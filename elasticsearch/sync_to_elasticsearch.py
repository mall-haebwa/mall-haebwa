#!/usr/bin/env python3
"""
MongoDB â†’ Elasticsearch ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸ (ë²¡í„° ì„ë² ë”© í¬í•¨)
- MongoDBì˜ products_v2 ì»¬ë ‰ì…˜ì„ Elasticsearchì˜ products_v2_vector ì¸ë±ìŠ¤ë¡œ ë™ê¸°í™”
- ì„ë² ë”©ì´ ìˆëŠ” ì œí’ˆë§Œ ë™ê¸°í™”
- ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ dense_vector í•„ë“œ í¬í•¨
"""
import os
import sys
import time
from pymongo import MongoClient
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from datetime import datetime
from typing import Dict, Any, List
from tqdm import tqdm

# MongoDB ì—°ê²°
MONGO_URL = os.getenv('MONGODB_URL', 'mongodb://54.116.25.146:27017/?replicaSet=rs0')
DB_NAME = os.getenv('MONGODB_DB_NAME', 'ecommerce_ai')

# Elasticsearch ì—°ê²°
ES_HOST = os.getenv('ELASTICSEARCH_URL', 'http://elasticsearch:9200')
VECTOR_INDEX = 'products_v2_vector'
EMBEDDING_DIMENSION = 1024

# MongoDB í´ë¼ì´ì–¸íŠ¸
mongo_client = MongoClient(MONGO_URL)
db = mongo_client[DB_NAME]
collection = db.products_v2

# Elasticsearch í´ë¼ì´ì–¸íŠ¸
es = Elasticsearch([ES_HOST])

def create_vector_index():
    """ë²¡í„° ê²€ìƒ‰ìš© Elasticsearch ì¸ë±ìŠ¤ ìƒì„±"""

    index_body = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 1,
            "analysis": {
                "analyzer": {
                    "korean_analyzer": {
                        "type": "custom",
                        "tokenizer": "nori_tokenizer",
                        "filter": ["lowercase", "nori_part_of_speech"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                # ìƒí’ˆ ê¸°ë³¸ ì •ë³´
                "product_id": {"type": "keyword"},
                "name": {
                    "type": "text",
                    "analyzer": "korean_analyzer",
                    "fields": {
                        "keyword": {"type": "keyword"}
                    }
                },
                "brand": {"type": "keyword"},
                "price": {"type": "integer"},
                "image_url": {"type": "keyword", "index": False},

                # ì¹´í…Œê³ ë¦¬ ì •ë³´
                "category_path": {"type": "keyword"},
                "category_full": {"type": "text", "analyzer": "korean_analyzer"},

                # ìƒì„¸ ì •ë³´
                "description_summary": {"type": "text", "analyzer": "korean_analyzer"},
                "text_content": {"type": "text", "analyzer": "korean_analyzer"},

                # í†µê³„ ì •ë³´
                "rating": {"type": "float"},
                "review_count": {"type": "integer"},
                "seller_name": {"type": "keyword"},
                "tags": {"type": "keyword"},

                # ë²¡í„° ì„ë² ë”©
                "embedding": {
                    "type": "dense_vector",
                    "dims": EMBEDDING_DIMENSION,
                    "index": True,
                    "similarity": "cosine"
                },

                # ë©”íƒ€ë°ì´í„°
                "embedding_model": {"type": "keyword"},
                "embedding_created_at": {"type": "date"},
                "synced_at": {"type": "date"}
            }
        }
    }

    if es.indices.exists(index=VECTOR_INDEX):
        print(f"âš ï¸  ì¸ë±ìŠ¤ '{VECTOR_INDEX}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        response = input("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() == 'y':
            es.indices.delete(index=VECTOR_INDEX)
            print(f"âœ“ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ")
        else:
            print("ê¸°ì¡´ ì¸ë±ìŠ¤ ìœ ì§€")
            return

    es.indices.create(index=VECTOR_INDEX, body=index_body)
    print(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ '{VECTOR_INDEX}' ìƒì„± ì™„ë£Œ!")


def prepare_es_document(product: Dict[str, Any]) -> Dict[str, Any]:
    """MongoDB ë¬¸ì„œë¥¼ Elasticsearch ë¬¸ì„œë¡œ ë³€í™˜"""

    # ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    category = product.get('category', {})
    category_path = []
    if category.get('level1'):
        category_path.append(category['level1'])
    if category.get('level2'):
        category_path.append(category['level2'])
    if category.get('level3'):
        category_path.append(category['level3'])
    if category.get('level4'):
        category_path.append(category['level4'])

    category_full = ' > '.join(category_path)

    # ì„¤ëª… ì²˜ë¦¬
    desc = product.get('description', {})
    desc_text = ''
    if isinstance(desc, dict):
        desc_text = desc.get('text', desc.get('summary', ''))
    elif isinstance(desc, str):
        desc_text = desc

    # í…ìŠ¤íŠ¸ ì½˜í…ì¸  êµ¬ì„± (ê²€ìƒ‰ìš©)
    text_parts = [
        product.get('name', ''),
        product.get('brand', ''),
        category_full,
        desc_text[:500] if desc_text else '',
        ' '.join(product.get('searchKeywords', {}).get('tags', []))
    ]
    text_content = ' '.join([p for p in text_parts if p])

    # Elasticsearch ë¬¸ì„œ êµ¬ì„±
    es_doc = {
        "product_id": str(product['_id']),
        "name": product.get('name', ''),
        "brand": product.get('brand', ''),
        "price": product.get('price', {}).get('amount', 0),
        "image_url": product.get('images', [{}])[0].get('url', ''),

        "category_path": category_path,
        "category_full": category_full,

        "description_summary": desc_text[:500] if desc_text else '',
        "text_content": text_content,

        "rating": product.get('ratings', {}).get('average', 0),
        "review_count": product.get('ratings', {}).get('count', 0),
        "seller_name": product.get('seller', {}).get('name', ''),
        "tags": product.get('searchKeywords', {}).get('tags', []),

        "embedding": product.get('embedding'),
        "embedding_model": product.get('embedding_model', ''),
        "embedding_created_at": product.get('embedding_created_at'),
        "synced_at": datetime.utcnow()
    }

    return es_doc


def generate_bulk_actions(batch: List[Dict], index_name: str):
    """Bulk APIìš© ì•¡ì…˜ ìƒì„±"""
    for product in batch:
        es_doc = prepare_es_document(product)
        yield {
            "_index": index_name,
            "_id": es_doc["product_id"],
            "_source": es_doc
        }


def sync_products_to_elasticsearch(batch_size: int = 1000, delay: float = 0.1):
    """MongoDB ì œí’ˆì„ Elasticsearchë¡œ ë™ê¸°í™” (ìµœì í™”)"""

    print("\n" + "=" * 60)
    print("MongoDB â†’ Elasticsearch ë™ê¸°í™” ì‹œì‘ (ìµœì í™” ëª¨ë“œ)")
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}, ë°°ì¹˜ë‹¹ ë”œë ˆì´: {delay}ì´ˆ")
    print("=" * 60)

    # í†µê³„ (estimated ì‚¬ìš©ìœ¼ë¡œ ë¶€í•˜ ê°ì†Œ)
    try:
        total = collection.estimated_document_count()
        with_embedding = collection.count_documents({'embedding': {'$exists': True, '$ne': None}})
    except Exception as e:
        print(f"âš ï¸  í†µê³„ ì¡°íšŒ ì‹¤íŒ¨, ì¶”ì •ê°’ ì‚¬ìš©: {e}")
        total = 0
        with_embedding = 0

    print(f"ì´ ì œí’ˆ: {total:,}")
    print(f"ì„ë² ë”© ìˆìŒ: {with_embedding:,} ({with_embedding/total*100:.1f}%)")
    print(f"ë™ê¸°í™” ëŒ€ìƒ: {with_embedding:,}")
    print("=" * 60)

    if with_embedding == 0:
        print("âŒ ì„ë² ë”©ì´ ìˆëŠ” ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”.")
        return

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë™ê¸°í™”
    batch = []
    success_count = 0
    error_count = 0

    cursor = collection.find(
        {'embedding': {'$exists': True, '$ne': None}},
        batch_size=batch_size
    )

    pbar = tqdm(total=with_embedding, desc="ES ë™ê¸°í™”")

    for product in cursor:
        batch.append(product)

        if len(batch) >= batch_size:
            try:
                success, errors = bulk(
                    es,
                    generate_bulk_actions(batch, VECTOR_INDEX),
                    raise_on_error=False,
                    request_timeout=60
                )
                success_count += success
                if errors:
                    error_count += len(errors)
                    print(f"\nâš ï¸  ë°°ì¹˜ì—ì„œ {len(errors)}ê°œ ì˜¤ë¥˜ ë°œìƒ")

                pbar.update(len(batch))
                batch = []

                # MongoDB ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(delay)
            except Exception as e:
                print(f"\nâŒ Bulk ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                error_count += len(batch)
                batch = []

    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if batch:
        try:
            success, errors = bulk(
                es,
                generate_bulk_actions(batch, VECTOR_INDEX),
                raise_on_error=False,
                request_timeout=60
            )
            success_count += success
            if errors:
                error_count += len(errors)
            pbar.update(len(batch))
        except Exception as e:
            print(f"\nâŒ Bulk ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
            error_count += len(batch)

    pbar.close()

    # ìµœì¢… í†µê³„
    print("\n" + "=" * 60)
    print("âœ… ë™ê¸°í™” ì™„ë£Œ!")
    print("=" * 60)
    print(f"ì„±ê³µ: {success_count:,}ê°œ")
    print(f"ì‹¤íŒ¨: {error_count:,}ê°œ")

    # ES ì¸ë±ìŠ¤ í†µê³„
    es.indices.refresh(index=VECTOR_INDEX)
    es_count = es.count(index=VECTOR_INDEX)['count']
    print(f"ES ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {es_count:,}ê°œ")
    print("=" * 60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    try:
        print("ğŸš€ Elasticsearch ë²¡í„° ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹œì‘\n")

        # ê°•ì œ ì¬ìƒì„± í”Œë˜ê·¸ í™•ì¸
        force_recreate = '--force' in sys.argv or '-f' in sys.argv

        # 1. ì¸ë±ìŠ¤ ìƒì„±
        if force_recreate:
            if es.indices.exists(index=VECTOR_INDEX):
                print(f"âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ '{VECTOR_INDEX}' ì‚­ì œ ì¤‘...")
                es.indices.delete(index=VECTOR_INDEX)
                print(f"âœ“ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ")

            index_body = {
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 1,
                    "analysis": {
                        "analyzer": {
                            "korean_analyzer": {
                                "type": "custom",
                                "tokenizer": "nori_tokenizer",
                                "filter": ["lowercase", "nori_part_of_speech"]
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        "product_id": {"type": "keyword"},
                        "name": {
                            "type": "text",
                            "analyzer": "korean_analyzer",
                            "fields": {"keyword": {"type": "keyword"}}
                        },
                        "brand": {"type": "keyword"},
                        "price": {"type": "integer"},
                        "image_url": {"type": "keyword", "index": False},
                        "category_path": {"type": "keyword"},
                        "category_full": {"type": "text", "analyzer": "korean_analyzer"},
                        "description_summary": {"type": "text", "analyzer": "korean_analyzer"},
                        "text_content": {"type": "text", "analyzer": "korean_analyzer"},
                        "rating": {"type": "float"},
                        "review_count": {"type": "integer"},
                        "seller_name": {"type": "keyword"},
                        "tags": {"type": "keyword"},
                        "embedding": {
                            "type": "dense_vector",
                            "dims": EMBEDDING_DIMENSION,
                            "index": True,
                            "similarity": "cosine"
                        },
                        "embedding_model": {"type": "keyword"},
                        "embedding_created_at": {"type": "date"},
                        "synced_at": {"type": "date"}
                    }
                }
            }
            es.indices.create(index=VECTOR_INDEX, body=index_body)
            print(f"âœ… ë²¡í„° ì¸ë±ìŠ¤ '{VECTOR_INDEX}' ìƒì„± ì™„ë£Œ!")
        else:
            create_vector_index()

        # 2. ì œí’ˆ ë™ê¸°í™”
        sync_products_to_elasticsearch()

        print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

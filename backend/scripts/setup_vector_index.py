#!/usr/bin/env python3
"""
Elasticsearch ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
products_v2_vector ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  dense_vector ë§¤í•‘ì„ ì„¤ì •í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
import json
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import RequestError
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env.docker")
load_dotenv(PROJECT_ROOT / ".env", override=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorIndexManager:
    """Elasticsearch ë²¡í„° ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™” ë° Elasticsearch ì—°ê²°"""
        self.es_url = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
        self.vector_index = os.getenv("ELASTICSEARCH_VECTOR_INDEX", "products_v2_vector")
        self.embedding_dim = int(os.getenv("EMBEDDING_DIMENSION", "1536"))

        logger.info(f"Elasticsearch URL: {self.es_url}")
        logger.info(f"Vector Index: {self.vector_index}")
        logger.info(f"Embedding Dimension: {self.embedding_dim}")

        # Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        self.es = Elasticsearch([self.es_url])

        # ì—°ê²° í™•ì¸
        if not self.es.ping():
            raise ConnectionError(f"Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.es_url}")

        logger.info("Elasticsearch ì—°ê²° ì„±ê³µ")

    def get_index_mapping(self) -> Dict[str, Any]:
        """ë²¡í„° ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜"""
        return {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "analysis": {
                    "analyzer": {
                        # ê¸°ì¡´ í•œêµ­ì–´ ë¶„ì„ê¸° ì‚¬ìš©
                        "korean_analyzer": {
                            "type": "custom",
                            "tokenizer": "nori_tokenizer",
                            "filter": [
                                "nori_part_of_speech",
                                "lowercase",
                                "nori_number"
                            ]
                        },
                        "korean_ngram_analyzer": {
                            "type": "custom",
                            "tokenizer": "nori_tokenizer",
                            "filter": [
                                "nori_part_of_speech",
                                "lowercase",
                                "edge_ngram_filter"
                            ]
                        }
                    },
                    "filter": {
                        "edge_ngram_filter": {
                            "type": "edge_ngram",
                            "min_gram": 1,
                            "max_gram": 10
                        },
                        "nori_part_of_speech": {
                            "type": "nori_part_of_speech",
                            "stoptags": [
                                "E", "IC", "J", "MAG", "MAJ",
                                "MM", "SP", "SSC", "SSO",
                                "SC", "SE", "XPN", "XSA",
                                "XSN", "XSV", "UNA", "NA",
                                "VSV"
                            ]
                        },
                        "nori_number": {
                            "type": "nori_number"
                        }
                    },
                    "tokenizer": {
                        "nori_tokenizer": {
                            "type": "nori_tokenizer",
                            "decompound_mode": "mixed"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    # ê¸°ë³¸ ì‹ë³„ì
                    "product_id": {
                        "type": "keyword"
                    },
                    "mongodb_id": {
                        "type": "keyword"
                    },

                    # ë²¡í„° ì„ë² ë”© (í•µì‹¬)
                    "embedding": {
                        "type": "dense_vector",
                        "dims": self.embedding_dim,
                        "index": True,
                        "similarity": "cosine"
                    },

                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš© í•„ë“œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
                    "text_content": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {
                            "ngram": {
                                "type": "text",
                                "analyzer": "korean_ngram_analyzer"
                            },
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    },

                    # ìƒí’ˆ ê¸°ë³¸ ì •ë³´
                    "name": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            },
                            "ngram": {
                                "type": "text",
                                "analyzer": "korean_ngram_analyzer"
                            }
                        }
                    },
                    "brand": {
                        "type": "keyword",
                        "fields": {
                            "text": {
                                "type": "text",
                                "analyzer": "korean_analyzer"
                            }
                        }
                    },

                    # ì¹´í…Œê³ ë¦¬
                    "category_path": {
                        "type": "keyword"
                    },
                    "category_full": {
                        "type": "text",
                        "analyzer": "korean_analyzer"
                    },

                    # íƒœê·¸ ë° í‚¤ì›Œë“œ
                    "tags": {
                        "type": "keyword",
                        "fields": {
                            "text": {
                                "type": "text",
                                "analyzer": "korean_analyzer"
                            }
                        }
                    },
                    "features": {
                        "type": "keyword"
                    },
                    "target_audience": {
                        "type": "keyword"
                    },
                    "use_case": {
                        "type": "keyword"
                    },

                    # ì„¤ëª…
                    "description_summary": {
                        "type": "text",
                        "analyzer": "korean_analyzer"
                    },

                    # ê°€ê²© ë° í‰ì  (í•„í„°ë§/ì •ë ¬ìš©)
                    "price": {
                        "type": "long"
                    },
                    "rating": {
                        "type": "float"
                    },
                    "review_count": {
                        "type": "integer"
                    },
                    "rank": {
                        "type": "integer"
                    },

                    # íŒë§¤ì ì •ë³´
                    "seller_name": {
                        "type": "keyword"
                    },
                    "seller_type": {
                        "type": "keyword"
                    },

                    # ì´ë¯¸ì§€
                    "image_url": {
                        "type": "keyword",
                        "index": False
                    },

                    # ë©”íƒ€ë°ì´í„°
                    "source_provider": {
                        "type": "keyword"
                    },
                    "source_product_id": {
                        "type": "keyword"
                    },
                    "search_keyword": {
                        "type": "keyword"
                    },

                    # ì„ë² ë”© ê´€ë ¨ ë©”íƒ€ë°ì´í„°
                    "embedding_model": {
                        "type": "keyword"
                    },
                    "embedding_created_at": {
                        "type": "date"
                    },
                    "embedding_version": {
                        "type": "keyword"
                    },

                    # íƒ€ì„ìŠ¤íƒ¬í”„
                    "created_at": {
                        "type": "date"
                    },
                    "updated_at": {
                        "type": "date"
                    },

                    # ìƒíƒœ
                    "status": {
                        "type": "keyword"
                    },
                    "published": {
                        "type": "boolean"
                    }
                }
            }
        }

    def index_exists(self) -> bool:
        """ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        return self.es.indices.exists(index=self.vector_index)

    def delete_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ"""
        try:
            if self.index_exists():
                response = self.es.indices.delete(index=self.vector_index)
                logger.info(f"ì¸ë±ìŠ¤ '{self.vector_index}' ì‚­ì œ ì™„ë£Œ")
                return True
            else:
                logger.info(f"ì¸ë±ìŠ¤ '{self.vector_index}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì‚­ì œ ì‹¤íŒ¨: {e}")
            raise

    def create_index(self, force: bool = False) -> bool:
        """ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±

        Args:
            force: Trueì¸ ê²½ìš° ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
        """
        try:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if self.index_exists():
                if not force:
                    logger.warning(f"ì¸ë±ìŠ¤ '{self.vector_index}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                    response = input("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“œì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
                    if response.lower() != 'y':
                        logger.info("ì¸ë±ìŠ¤ ìƒì„± ì·¨ì†Œ")
                        return False

                # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
                self.delete_index()

            # ì¸ë±ìŠ¤ ìƒì„±
            mapping = self.get_index_mapping()
            response = self.es.indices.create(
                index=self.vector_index,
                body=mapping
            )

            if response.get('acknowledged'):
                logger.info(f"ì¸ë±ìŠ¤ '{self.vector_index}' ìƒì„± ì™„ë£Œ!")
                logger.info(f"- Embedding ì°¨ì›: {self.embedding_dim}")
                logger.info(f"- Similarity: cosine")
                logger.info(f"- Shards: 2, Replicas: 1")
                return True
            else:
                logger.error("ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                return False

        except RequestError as e:
            if 'resource_already_exists_exception' in str(e):
                logger.error(f"ì¸ë±ìŠ¤ '{self.vector_index}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
            else:
                logger.error(f"ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
            raise
        except Exception as e:
            logger.error(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
            raise

    def get_index_info(self) -> Optional[Dict[str, Any]]:
        """ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        try:
            if not self.index_exists():
                logger.warning(f"ì¸ë±ìŠ¤ '{self.vector_index}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return None

            # ì¸ë±ìŠ¤ ë§¤í•‘ ì¡°íšŒ
            mapping = self.es.indices.get_mapping(index=self.vector_index)

            # ì¸ë±ìŠ¤ ì„¤ì • ì¡°íšŒ
            settings = self.es.indices.get_settings(index=self.vector_index)

            # ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ
            stats = self.es.indices.stats(index=self.vector_index)

            doc_count = stats['indices'][self.vector_index]['primaries']['docs']['count']
            size_in_bytes = stats['indices'][self.vector_index]['primaries']['store']['size_in_bytes']
            size_in_mb = size_in_bytes / (1024 * 1024)

            info = {
                "index_name": self.vector_index,
                "document_count": doc_count,
                "size_mb": round(size_in_mb, 2),
                "shards": settings[self.vector_index]['settings']['index']['number_of_shards'],
                "replicas": settings[self.vector_index]['settings']['index']['number_of_replicas'],
                "embedding_dims": None
            }

            # ë²¡í„° ì°¨ì› í™•ì¸
            properties = mapping[self.vector_index]['mappings'].get('properties', {})
            if 'embedding' in properties:
                embedding_config = properties['embedding']
                if embedding_config.get('type') == 'dense_vector':
                    info['embedding_dims'] = embedding_config.get('dims')

            return info

        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def test_vector_search(self) -> None:
        """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ë°ì´í„° ì‚¬ìš©)"""
        try:
            if not self.index_exists():
                logger.warning("ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
                return

            # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
            import numpy as np

            test_doc = {
                "product_id": "test_product_001",
                "mongodb_id": "507f1f77bcf86cd799439011",
                "name": "í…ŒìŠ¤íŠ¸ ìš´ë™í™”",
                "brand": "ë‚˜ì´í‚¤",
                "category_path": ["íŒ¨ì…˜ì˜ë¥˜", "ì‹ ë°œ", "ìš´ë™í™”"],
                "category_full": "íŒ¨ì…˜ì˜ë¥˜ > ì‹ ë°œ > ìš´ë™í™”",
                "tags": ["ìš´ë™í™”", "ëŸ°ë‹í™”", "ìŠ¤í¬ì¸ "],
                "features": ["ì¿ ì…˜", "ê²½ëŸ‰", "í†µê¸°ì„±"],
                "target_audience": ["20ëŒ€", "30ëŒ€", "ë‚¨ì„±"],
                "use_case": ["ëŸ¬ë‹", "ì¡°ê¹…", "ì¼ìƒ"],
                "description_summary": "í¸ì•ˆí•œ ì°©í™”ê°ê³¼ ë›°ì–´ë‚œ ì¿ ì…˜ì„ ì œê³µí•˜ëŠ” ëŸ°ë‹í™”",
                "text_content": "ë‚˜ì´í‚¤ í…ŒìŠ¤íŠ¸ ìš´ë™í™” ëŸ°ë‹í™” ìŠ¤í¬ì¸  ì¿ ì…˜ ê²½ëŸ‰ í†µê¸°ì„±",
                "embedding": np.random.rand(self.embedding_dim).tolist(),  # ëœë¤ ë²¡í„°
                "price": 89000,
                "rating": 4.5,
                "review_count": 234,
                "rank": 1,
                "seller_name": "ë‚˜ì´í‚¤ ê³µì‹ìŠ¤í† ì–´",
                "seller_type": "official",
                "embedding_model": "amazon.titan-embed-text-v2:0",
                "embedding_created_at": datetime.utcnow().isoformat(),
                "embedding_version": "1.0",
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat(),
                "status": "published",
                "published": True
            }

            # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹±
            response = self.es.index(
                index=self.vector_index,
                id="test_001",
                body=test_doc,
                refresh=True
            )

            if response['result'] in ['created', 'updated']:
                logger.info("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ")

                # KNN ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                query_vector = np.random.rand(self.embedding_dim).tolist()

                search_query = {
                    "knn": {
                        "field": "embedding",
                        "query_vector": query_vector,
                        "k": 10,
                        "num_candidates": 100
                    },
                    "_source": ["product_id", "name", "brand", "price"]
                }

                search_response = self.es.search(
                    index=self.vector_index,
                    body=search_query
                )

                logger.info(f"KNN ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ê²°ê³¼: {search_response['hits']['total']['value']}ê°œ")

                # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì‚­ì œ
                self.es.delete(index=self.vector_index, id="test_001")
                logger.info("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")

            else:
                logger.error("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨")

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    def print_summary(self) -> None:
        """ì¸ë±ìŠ¤ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        info = self.get_index_info()
        if info:
            print("\n" + "="*60)
            print(f"ğŸ“Š Elasticsearch Vector Index ì •ë³´")
            print("="*60)
            print(f"ì¸ë±ìŠ¤ëª…: {info['index_name']}")
            print(f"ë¬¸ì„œ ìˆ˜: {info['document_count']:,}ê°œ")
            print(f"í¬ê¸°: {info['size_mb']} MB")
            print(f"ìƒ¤ë“œ: {info['shards']}, ë ˆí”Œë¦¬ì¹´: {info['replicas']}")
            print(f"ì„ë² ë”© ì°¨ì›: {info['embedding_dims']}")
            print("="*60)
        else:
            print(f"\nâŒ ì¸ë±ìŠ¤ '{self.vector_index}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='Elasticsearch ë²¡í„° ì¸ë±ìŠ¤ ì„¤ì •')
    parser.add_argument('--create', action='store_true', help='ì¸ë±ìŠ¤ ìƒì„±')
    parser.add_argument('--delete', action='store_true', help='ì¸ë±ìŠ¤ ì‚­ì œ')
    parser.add_argument('--force', action='store_true', help='ê°•ì œ ì‹¤í–‰ (í™•ì¸ ì—†ì´)')
    parser.add_argument('--info', action='store_true', help='ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ')
    parser.add_argument('--test', action='store_true', help='ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸')

    args = parser.parse_args()

    try:
        manager = VectorIndexManager()

        if args.delete:
            if args.force or input(f"ì •ë§ '{manager.vector_index}' ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y':
                manager.delete_index()

        elif args.create:
            manager.create_index(force=args.force)
            if args.test:
                print("\në²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
                manager.test_vector_search()

        elif args.info:
            manager.print_summary()

        elif args.test:
            manager.test_vector_search()

        else:
            # ê¸°ë³¸: ì •ë³´ í‘œì‹œ
            manager.print_summary()
            print("\nì‚¬ìš©ë²•:")
            print("  --create : ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
            print("  --delete : ì¸ë±ìŠ¤ ì‚­ì œ")
            print("  --info   : ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ")
            print("  --test   : ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            print("  --force  : í™•ì¸ ì—†ì´ ê°•ì œ ì‹¤í–‰")
            print(f"\nì˜ˆ: python {Path(__file__).name} --create --test")

    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
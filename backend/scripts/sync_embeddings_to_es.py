#!/usr/bin/env python3
"""
MongoDBì—ì„œ Elasticsearchë¡œ ì„ë² ë”© ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸
products_v2ì˜ ì„ë² ë”© ë°ì´í„°ë¥¼ products_v2_vector ì¸ë±ìŠ¤ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import logging
from dataclasses import dataclass
from collections import defaultdict
import threading

import pymongo
from pymongo import MongoClient
from elasticsearch import Elasticsearch, helpers
from elasticsearch.exceptions import BulkIndexError
import redis
from dotenv import load_dotenv
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env.docker")
load_dotenv(PROJECT_ROOT / ".env", override=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class SyncConfig:
    """ë™ê¸°í™” ì„¤ì • í´ë˜ìŠ¤"""
    batch_size: int = 1000
    max_retries: int = 3
    retry_delay: int = 2
    enable_change_stream: bool = True
    sync_interval: int = 60  # seconds


class EmbeddingSynchronizer:
    """ì„ë² ë”© ë™ê¸°í™” í´ë˜ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.config = SyncConfig()

        # MongoDB ì—°ê²°
        mongo_url = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
        db_name = os.getenv("MONGODB_DB_NAME", "ecommerce_ai")
        self.mongo_client = MongoClient(mongo_url)
        self.db = self.mongo_client[db_name]
        self.products_collection = self.db["products_v2"]

        # Elasticsearch ì—°ê²°
        es_url = os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
        self.vector_index = os.getenv("ELASTICSEARCH_VECTOR_INDEX", "products_v2_vector")
        self.es = Elasticsearch([es_url])

        if not self.es.ping():
            raise ConnectionError(f"Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {es_url}")

        # Redis ì—°ê²° (ì§„í–‰ ìƒí™© ì¶”ì )
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        self.redis_client = redis.from_url(redis_url, decode_responses=True)

        # ì„¸ì…˜ ID ìƒì„±
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        logger.info(f"ë™ê¸°í™” ì‹œì‘ - Session ID: {self.session_id}")

    def create_es_document(self, product: Dict[str, Any]) -> Dict[str, Any]:
        """MongoDB ë¬¸ì„œë¥¼ Elasticsearch ë¬¸ì„œë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ í•„ë“œ
            es_doc = {
                "product_id": str(product["_id"]),
                "mongodb_id": str(product["_id"]),
                "name": product.get("name", ""),
                "brand": product.get("brand", ""),
                "created_at": product.get("createdAt"),
                "updated_at": product.get("updatedAt"),
                "status": product.get("status", {}).get("processed"),
                "published": product.get("status", {}).get("published"),
            }

            # ì„ë² ë”© ë²¡í„°
            if "embedding_vector" in product:
                es_doc["embedding"] = product["embedding_vector"]
                es_doc["embedding_model"] = product.get("embedding_model", "")
                es_doc["embedding_created_at"] = product.get("embedding_created_at")
                es_doc["embedding_version"] = product.get("embedding_dimension", "1.0")

            # ì„ë² ë”© í…ìŠ¤íŠ¸ (ê²€ìƒ‰ìš©)
            if "embedding_text" in product:
                es_doc["text_content"] = product["embedding_text"]

            # ì¹´í…Œê³ ë¦¬
            if category := product.get("category", {}):
                if path := category.get("path"):
                    es_doc["category_path"] = path
                    es_doc["category_full"] = " > ".join(path)

            # ê²€ìƒ‰ í‚¤ì›Œë“œ
            if search_keywords := product.get("searchKeywords", {}):
                es_doc["tags"] = search_keywords.get("tags", [])
                es_doc["features"] = search_keywords.get("features", [])
                es_doc["target_audience"] = search_keywords.get("targetAudience", [])
                es_doc["use_case"] = search_keywords.get("useCase", [])

            # ì„¤ëª…
            if description := product.get("description", {}):
                es_doc["description_summary"] = description.get("summary", "")

            # ê°€ê²©
            if price_info := product.get("price", {}):
                es_doc["price"] = price_info.get("current", 0)

            # ë¦¬ë·°
            if reviews := product.get("reviews", {}):
                es_doc["rating"] = reviews.get("rating", 0.0)
                es_doc["review_count"] = reviews.get("count", 0)

            # ìˆœìœ„
            if source := product.get("source", {}):
                es_doc["rank"] = source.get("rank", 999999)
                es_doc["source_provider"] = source.get("provider", "")
                es_doc["source_product_id"] = source.get("productId", "")
                es_doc["search_keyword"] = source.get("searchKeyword", "")

            # íŒë§¤ì
            if seller := product.get("seller", {}):
                es_doc["seller_name"] = seller.get("name", "")
                es_doc["seller_type"] = seller.get("type", "")

            # ì´ë¯¸ì§€
            if images := product.get("images", {}):
                es_doc["image_url"] = images.get("thumbnail", "")

            return es_doc

        except Exception as e:
            logger.error(f"ES ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def get_products_with_embeddings(self, last_id: Optional[str] = None) -> pymongo.cursor.Cursor:
        """ì„ë² ë”©ì´ ìˆëŠ” ìƒí’ˆ ì¡°íšŒ"""
        query = {
            "embedding_vector": {"$exists": True},
            "embedding_status": "completed"
        }

        if last_id:
            query["_id"] = {"$gt": pymongo.ObjectId(last_id)}

        return self.products_collection.find(query).sort("_id", 1)

    def bulk_index(self, documents: List[Dict[str, Any]]) -> Tuple[int, int]:
        """ë²Œí¬ ì¸ë±ì‹±

        Returns:
            (ì„±ê³µ ìˆ˜, ì‹¤íŒ¨ ìˆ˜)
        """
        if not documents:
            return 0, 0

        actions = []
        for doc in documents:
            if doc:
                actions.append({
                    "_index": self.vector_index,
                    "_id": doc["product_id"],
                    "_source": doc
                })

        if not actions:
            return 0, 0

        try:
            success, failed = helpers.bulk(
                self.es,
                actions,
                raise_on_error=False,
                raise_on_exception=False,
                stats_only=True
            )

            if failed > 0:
                logger.warning(f"ë²Œí¬ ì¸ë±ì‹± ì¼ë¶€ ì‹¤íŒ¨ - ì„±ê³µ: {success}, ì‹¤íŒ¨: {failed}")

            return success, failed

        except BulkIndexError as e:
            logger.error(f"ë²Œí¬ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
            # ì¼ë¶€ ì„±ê³µí•œ ê²½ìš° ì²˜ë¦¬
            success = len(actions) - len(e.errors)
            return success, len(e.errors)

        except Exception as e:
            logger.error(f"ë²Œí¬ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            return 0, len(actions)

    def initial_sync(self) -> None:
        """ì´ˆê¸° ì „ì²´ ë™ê¸°í™”"""
        logger.info("ì´ˆê¸° ë™ê¸°í™” ì‹œì‘...")

        # ì „ì²´ ê°œìˆ˜ í™•ì¸
        total_count = self.products_collection.count_documents({
            "embedding_vector": {"$exists": True},
            "embedding_status": "completed"
        })

        if total_count == 0:
            logger.info("ë™ê¸°í™”í•  ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"ë™ê¸°í™”í•  ìƒí’ˆ: {total_count:,}ê°œ")

        # ì§„í–‰ ìƒí™© ì¶”ì 
        progress = {
            "session_id": self.session_id,
            "type": "initial_sync",
            "processed": 0,
            "success": 0,
            "failed": 0,
            "total": total_count,
            "start_time": datetime.utcnow().isoformat()
        }

        cursor = self.get_products_with_embeddings()

        batch = []
        total_processed = 0
        total_success = 0
        total_failed = 0

        start_time = time.time()

        with tqdm(total=total_count, desc="Elasticsearch ë™ê¸°í™”") as pbar:
            for product in cursor:
                # ES ë¬¸ì„œ ìƒì„±
                es_doc = self.create_es_document(product)
                if es_doc:
                    batch.append(es_doc)

                # ë°°ì¹˜ê°€ ê°€ë“ ì°¨ë©´ ì¸ë±ì‹±
                if len(batch) >= self.config.batch_size:
                    success, failed = self.bulk_index(batch)

                    total_success += success
                    total_failed += failed
                    total_processed += len(batch)

                    pbar.update(len(batch))

                    # ì§„í–‰ ìƒí™© ì €ì¥
                    progress.update({
                        "processed": total_processed,
                        "success": total_success,
                        "failed": total_failed,
                        "last_id": str(product["_id"]),
                        "elapsed_time": int(time.time() - start_time)
                    })
                    self.save_progress(progress)

                    # ë¡œê·¸
                    if total_processed % 10000 == 0:
                        rate = total_processed / (time.time() - start_time)
                        logger.info(
                            f"ì§„í–‰: {total_processed:,}/{total_count:,} "
                            f"({total_processed/total_count*100:.1f}%) "
                            f"ì†ë„: {rate:.1f}/s"
                        )

                    batch = []

            # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
            if batch:
                success, failed = self.bulk_index(batch)
                total_success += success
                total_failed += failed
                total_processed += len(batch)
                pbar.update(len(batch))

        # ì™„ë£Œ
        elapsed_time = time.time() - start_time
        progress.update({
            "status": "completed",
            "processed": total_processed,
            "success": total_success,
            "failed": total_failed,
            "elapsed_time": int(elapsed_time),
            "completed_at": datetime.utcnow().isoformat()
        })
        self.save_progress(progress)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("âœ… ì´ˆê¸° ë™ê¸°í™” ì™„ë£Œ!")
        print("="*60)
        print(f"ì„¸ì…˜ ID: {self.session_id}")
        print(f"ì²˜ë¦¬ ë¬¸ì„œ: {total_processed:,}ê°œ")
        print(f"ì„±ê³µ: {total_success:,}ê°œ")
        print(f"ì‹¤íŒ¨: {total_failed:,}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
        print(f"í‰ê·  ì†ë„: {total_processed/elapsed_time:.1f}ê°œ/ì´ˆ")
        print("="*60)

    def watch_changes(self) -> None:
        """Change Streamì„ í†µí•œ ì‹¤ì‹œê°„ ë™ê¸°í™”"""
        logger.info("ì‹¤ì‹œê°„ ë™ê¸°í™” ì‹œì‘ (Change Stream)...")

        # Change Stream íŒŒì´í”„ë¼ì¸
        pipeline = [
            {
                "$match": {
                    "operationType": {"$in": ["insert", "update", "replace"]},
                    "$or": [
                        {"fullDocument.embedding_vector": {"$exists": True}},
                        {"updateDescription.updatedFields.embedding_vector": {"$exists": True}}
                    ]
                }
            }
        ]

        try:
            with self.products_collection.watch(pipeline) as stream:
                logger.info("Change Stream í™œì„±í™”ë¨")

                for change in stream:
                    try:
                        operation = change["operationType"]
                        document_id = change["documentKey"]["_id"]

                        logger.info(f"ë³€ê²½ ê°ì§€ - Type: {operation}, ID: {document_id}")

                        if operation in ["insert", "update", "replace"]:
                            # ì „ì²´ ë¬¸ì„œ ì¡°íšŒ
                            product = self.products_collection.find_one({"_id": document_id})

                            if product and "embedding_vector" in product:
                                # ES ë¬¸ì„œ ìƒì„± ë° ì¸ë±ì‹±
                                es_doc = self.create_es_document(product)
                                if es_doc:
                                    response = self.es.index(
                                        index=self.vector_index,
                                        id=es_doc["product_id"],
                                        body=es_doc
                                    )

                                    if response["result"] in ["created", "updated"]:
                                        logger.info(f"ë™ê¸°í™” ì™„ë£Œ - ID: {document_id}")
                                    else:
                                        logger.warning(f"ë™ê¸°í™” ì‹¤íŒ¨ - ID: {document_id}")

                    except Exception as e:
                        logger.error(f"Change ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

        except pymongo.errors.OperationFailure as e:
            if "The $changeStream stage is only supported on replica sets" in str(e):
                logger.error("Change Streamì€ Replica Setì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
                logger.info("Replica Set ì„¤ì • í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            else:
                logger.error(f"Change Stream ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ë™ê¸°í™” ì˜¤ë¥˜: {e}")

    def verify_sync(self) -> Dict[str, Any]:
        """ë™ê¸°í™” ê²€ì¦"""
        logger.info("ë™ê¸°í™” ê²€ì¦ ì‹œì‘...")

        # MongoDB í†µê³„
        mongo_stats = {
            "total": self.products_collection.count_documents({}),
            "with_embedding": self.products_collection.count_documents({
                "embedding_vector": {"$exists": True},
                "embedding_status": "completed"
            })
        }

        # Elasticsearch í†µê³„
        es_response = self.es.count(index=self.vector_index)
        es_count = es_response.get("count", 0)

        # ìƒ˜í”Œ ê²€ì¦ (ì„ë² ë”© ì°¨ì›)
        es_sample = self.es.search(
            index=self.vector_index,
            size=1,
            body={"query": {"exists": {"field": "embedding"}}}
        )

        embedding_dim = None
        if es_sample["hits"]["hits"]:
            sample_doc = es_sample["hits"]["hits"][0]["_source"]
            if "embedding" in sample_doc:
                embedding_dim = len(sample_doc["embedding"])

        # ê²°ê³¼ ë°˜í™˜
        return {
            "mongo_total": mongo_stats["total"],
            "mongo_with_embedding": mongo_stats["with_embedding"],
            "es_count": es_count,
            "sync_rate": es_count / mongo_stats["with_embedding"] * 100 if mongo_stats["with_embedding"] > 0 else 0,
            "embedding_dimension": embedding_dim
        }

    def save_progress(self, progress: Dict[str, Any]) -> None:
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        key = f"sync:progress:{self.session_id}"
        self.redis_client.setex(
            key,
            86400,  # 24ì‹œê°„ TTL
            json.dumps(progress)
        )

    def print_verification_result(self, stats: Dict[str, Any]) -> None:
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ë™ê¸°í™” ê²€ì¦ ê²°ê³¼")
        print("="*60)
        print(f"MongoDB:")
        print(f"  - ì „ì²´ ìƒí’ˆ: {stats['mongo_total']:,}ê°œ")
        print(f"  - ì„ë² ë”© ìˆìŒ: {stats['mongo_with_embedding']:,}ê°œ")
        print(f"\nElasticsearch:")
        print(f"  - ì¸ë±ìŠ¤ ë¬¸ì„œ: {stats['es_count']:,}ê°œ")
        print(f"  - ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}")
        print(f"\në™ê¸°í™”ìœ¨: {stats['sync_rate']:.1f}%")

        if stats['sync_rate'] < 100:
            diff = stats['mongo_with_embedding'] - stats['es_count']
            print(f"âš ï¸  ë™ê¸°í™” í•„ìš”: {diff:,}ê°œ ë¬¸ì„œ")
        else:
            print("âœ… ëª¨ë“  ì„ë² ë”©ì´ ë™ê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")

        print("="*60)

    def run_continuous_sync(self) -> None:
        """ì§€ì†ì ì¸ ë™ê¸°í™” ì‹¤í–‰"""
        logger.info("ì§€ì†ì ì¸ ë™ê¸°í™” ëª¨ë“œ ì‹œì‘...")

        # ì´ˆê¸° ë™ê¸°í™”
        self.initial_sync()

        # ì‹¤ì‹œê°„ ë™ê¸°í™” ì‹œì‘ (ë³„ë„ ìŠ¤ë ˆë“œ)
        if self.config.enable_change_stream:
            watch_thread = threading.Thread(target=self.watch_changes, daemon=True)
            watch_thread.start()
            logger.info("Change Stream ìŠ¤ë ˆë“œ ì‹œì‘")

        # ì£¼ê¸°ì ì¸ ê²€ì¦ ë° ì¬ë™ê¸°í™”
        try:
            while True:
                time.sleep(self.config.sync_interval)

                # ë™ê¸°í™” ê²€ì¦
                stats = self.verify_sync()

                if stats['sync_rate'] < 100:
                    logger.info(f"ë™ê¸°í™”ìœ¨ {stats['sync_rate']:.1f}% - ì¬ë™ê¸°í™” ì‹œì‘...")
                    self.initial_sync()
                else:
                    logger.info(f"ë™ê¸°í™” ìƒíƒœ ì–‘í˜¸ - {stats['es_count']:,}ê°œ ë¬¸ì„œ")

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='ì„ë² ë”© Elasticsearch ë™ê¸°í™”')
    parser.add_argument('--initial', action='store_true', help='ì´ˆê¸° ì „ì²´ ë™ê¸°í™”')
    parser.add_argument('--watch', action='store_true', help='Change Stream ëª¨ë‹ˆí„°ë§')
    parser.add_argument('--continuous', action='store_true', help='ì§€ì†ì ì¸ ë™ê¸°í™”')
    parser.add_argument('--verify', action='store_true', help='ë™ê¸°í™” ê²€ì¦')
    parser.add_argument('--batch-size', type=int, default=1000, help='ë°°ì¹˜ í¬ê¸°')

    args = parser.parse_args()

    try:
        synchronizer = EmbeddingSynchronizer()

        if args.batch_size:
            synchronizer.config.batch_size = args.batch_size

        if args.verify:
            stats = synchronizer.verify_sync()
            synchronizer.print_verification_result(stats)

        elif args.initial:
            synchronizer.initial_sync()

        elif args.watch:
            synchronizer.watch_changes()

        elif args.continuous:
            synchronizer.run_continuous_sync()

        else:
            # ê¸°ë³¸: ì´ˆê¸° ë™ê¸°í™” í›„ ê²€ì¦
            synchronizer.initial_sync()
            stats = synchronizer.verify_sync()
            synchronizer.print_verification_result(stats)

    except KeyboardInterrupt:
        logger.info("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()